# Project 4: Report

Project 4 : Multi Paxos Implementation for a Key Value Store
## Intro (Your understanding)
The aim of the project is to maintain a consistent log in multiple replicas for excutions on a key value store. To do this we use MultiPaxos consensus algorithm and the implementation follows the paper Paxos Made Moderately Complex provided in reference(1).

## Design Decisions
PaxosServer class has three inner classes to manage the different roles that server takes in.
The three inner classes are LeaderNode, ReplicaNode, AcceptorNode. LeaderNode inturn has two inner classes Scout and
Commander to manage the prepare (P1a) and accept phase (P2a).
PaxosClient and AMOApplication implementation remain the same as previous project. The only difference is PaxosClient
sends the request to all the servers (i.e. broadcast the request)

ReplicaNode: This role is responsible for proposing and executing the AMOApplication upon receiving the decision. This role is 
responsible for identifying the empty slot in the log while proposing a slot for a request (slot_in). It also maintains the slot (slot_out) that indicates the request which is last executed.
  
AcceptorNode: This role is responsible for receiving P1aMessage and P1bMessage. It responds with a P1bMessage and P2bMessage respectively. The node maintains the highest ballot number it has so far seen and also maintains a list of proposals that were so far accepted by every server it is aware of.

LeaderNode: This is responsible for accepting the proposals from the replicas and sending the final decision on slot number of the log for the command. Only one of the server's leader node takes a decision. It does this using Commander objects (or Pvalue object) which send P2aMessages to all the server's acceptor roles. These acceptors would inturn send P2bMessages. Once it gets a reply from majority of acceptors, leader node sends DecisionMsg to all servers with final log slot for the request.

Messages:
PaxosRequest : message from client to server
ProposalMsg : message from all replicas to distinguished/active leader. Message contains proposed slot number for the request.
P1aMessage : message that starts the leader election process (prepare phase)
P2aMessage: message that initiates the collection of votes from other servers for the proposal message.
P1bMessage : reply for P1aMessages containing the highest ballot number so far seen by sender
P2bMessage : reply to P2aMessage containing the all the Pvalues so far been accepted by the sender server.
DecisionMsg: reply generated by distinguished leader about the slot number and is sent to all servers
HearbearMsg: generated by active leader and sent to all follower servers
LogStateMsg: generated by followers when they receive hearbeat message from about their slot out

Timers:
ClientTimer: timer to retry client request
HearbeatTimer: timer to resend the HeartbeatMsg at regular intervals
HeartbeatCheckTimer: timer to check if the server has got heartbeat message from leader. 
P1aTimer: timer to retry P1aMessage
P2aTimer: timer to retry P1bMessage
ProposeTimer: timer to retry ProposalMsg. This is to handle loss of ProposalMsg and DecisionMsg

ReplicatedLog: It is a map<Slot number, PaxosLogEntry>. PaxosLogEntry contains PaxosRequest, PaxosLogStatus.

Leader Election:
When a PaxosServer starts up, each of the roles are initialized. As part of the initialization, leader election process is initiated along with initializing the internal data structures. The leader election process is refered as P1a phase in the paper. The idea of this election process is to identify a distinguished leader among all the servers which decides the slots for each request in the log. Leader role in each server assumes a unique id of the form <counter, serverid>. We refer to this id as ballot number. The one with the highest id is elected as leader. The process starts by creating a Scout object with this ballot number. This scout object sends a P1aMessage to all the other servers acceptor roles to check if its ballot number is greater than the ballot number of other server.
The P1aMessage is processed by acceptor role in each server. Acceptor role maintains the highest ballot number it has seen so far in all the P1aMessage it receives. The acceptor role in each server replies with P1bMessage which contains the highest ballot number it has so far seen. Upon receiving the P1bMessage each scout sees if the ballot number is same as its ballot number. If the scout has received a message with highest ballot number than its ballot number, generates a PreemeptMessages and quits beign a leader. But if it receives the same ballot number as its own from a majority of servers then it would declare itself an active leader meaning it is ready to process ProposalMsg and provide DecisionMsg on the slot of the log. There may be more than one active leaders. It is by the next phase P2 the distinguished leader is identified among these active leaders and all other servers become followers. As a part of becoming a distinguished leader, it also gets all the accepted proposals (Pvalues) from all the servers. It would then repropose these if there is a pending decision on any of them.

Switching Leaders:
Active leaders generate HeartbeatMsg which is sent to all servers. If followers did not get any heartbeat message from the active leader in two consecutive HeartbeatCheckTimer intervals, one of the follower node would start a Scout and start the leader election process. At any point of time, when a server gets a P1aMessage or P2aMessage from another server whose ballot number is higher then its own, it would go to follower mode and accept the server with highest ballot number as distinguished leader.

## Flow of Control & Code Design
PaxosClient sends the request to all PaxosServers. Upon receiving the request by the replicanode of the server, we check if it is
a stale request. To do this we maintain a map of latest processing sequence number for each client. If the sequence number of incoming request is less than what is in the map, it is a stale request and we reject it. Since clients dont generate another request until a response is received for their current request, maintaining the current sequence number in this map is sufficient.
 
The requests from each client is then added to a queue. Each replica takes a request from the queue and tries to construct a proposal. ReplicaNode/role maintain a slot_in and slot_out which are indexes in the replicated log that we are trying to generate.

slot_in is the index in the log for next proposal to be made. All the slots below this point have either been decided or proposed.
slot_out denotes the index in the log uptill which the request have been executed in KVStore.
The slots between slot_in and slot_out are either decided or in the pipeline for being processed.

Upon receiving the request, we check if it is in the set of proposals that were already made. If it is not, then we pick the slot_in for the proposal and increment the slot_in. The proposal is then put in the map of proposals < slotnumber, PaxosRequest>.and forwarded to the leader role. The leadernode/role then forwards it to distinguished leader.

The distinguished leader upon receiving the ProposalMsg spawns a commander object(Pvalue) and sends P2aMessage to all acceptors. The acceptors would then send P2bMessage. They also add this proposal to a Pvalue set and mark the log entry for the slot as ACCEPTED. If leader receives a majority of P1bMessages with the same ballot number as its own, then the proposal is meant to be accepted. At this point distinguished leader sends the DecisionMsg to all servers (replica roles). If it is not accepted by majority then the leader is preempted and a new leader election is started.

The replicaNode/role then adds it to the decision list. If the decision is received for the current slot_out value, then the decision applied to log as well as AMOApplication meaning the application is executed and KVStore is modified. A new entry is put into Replicatedlog and entry is marked CHOSEN. If the decision is received for later slot than slot_out then we wait till the decision for slot_out is received.

Garbage Collection:
Each server sends a LogStateMessage to distinguished leader which contain information of slot_out value. This is used by the leader to see what the minimum slot_out value is in all servers. The leader then inform all the servers to clear the log, decisions map and Pvalue set till that min slot out value which they do upon arrival of HeartbeatMsg which contains this value. The entries in log for these slots are marked CLEARED. If any follower nodes are lagging by more than 10 slots, the leader server sends a BatchDecisionMsg of all the missing entries to lagging server.

## Missing Components
Could not resolve long waittime tests and search state testcases.

## References
1. https://www.cs.cornell.edu/home/rvr/Paxos/paxos.pdf
